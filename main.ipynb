{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86973b25-537b-4202-b188-7c33faef203e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7d776-afcf-49a6-8de9-8af7fcc3745b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00989dec-2033-4820-8282-cb483f5e70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10dd7f-78f5-4a3b-bc8c-7d6e9781b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb268e33-a9fd-497d-831c-5571ddc098fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43dd33-0ad3-47ad-9639-358827e59062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('smile-annotations-final.csv', names=['id', 'text', 'category'])\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b9f8a-1821-4f88-9cf5-ec5694c7066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5021a-3340-4b3d-8584-58a79d1a79da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc83b5b-57d4-4437-a4ae-3ac4468bad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts() # it counts How many times each unique instance occur in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c69f9-f23a-4e8a-bfb5-30a02172c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the tweet with multiple category/nocode\n",
    "df = df[~df.category.str.contains('\\|')]\n",
    "                    #str -> As we have to pull-it out of the string\n",
    "                        #contain -> if the str contaion '|' -> Return True, Else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560f547-1aed-42c3-858f-992128f3102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.category != 'nocode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698ba10-e33b-47df-a57f-23abcf5dfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd281f-4f03-4ca1-b134-c2ada2bc8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = df.category.unique() # Now we have the list that conatin all-of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515febdc-b252-4915-8bde-8810d9a8512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {} # Creating an empty Dict, & Looping over the possible labels \n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369837f-8ad6-4470-833b-365ccd269111",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8a951-0654-4ccd-9e3d-3b53008da877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.category.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff744de0-41c9-4b02-8deb-0bc3f52c1604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4c4a6-ed03-43e4-9689-51851680342b",
   "metadata": {},
   "source": [
    "### Training/Validation Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab07e9e-ca3c-4db7-91a9-c06d2ff8c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adea49-21c8-48d6-a3d5-0df3ac82a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val =  train_test_split(df.index.values,\n",
    "                                                   df.label.values,\n",
    "                                                   test_size=0.15,\n",
    "                                                   random_state=17,\n",
    "                                                   stratify=df.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14f3d9-ad4f-478f-a964-1479a2a7aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the New column in our dataframe --> 'data_type'\n",
    "# data_type is Initally 'not_set' for all the samples\n",
    "df['data_type'] = ['not_set']*df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b97067-e9fb-4910-be9a-58bbf32ab7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a462a-828c-411b-9f23-5ccc583ac677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[x_train, 'data_type'] = 'train'\n",
    "df.loc[x_val, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd2751-1dcc-4ac8-ab80-5c2a7a2a7f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['category', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625dbf-8fb2-4d3f-aaba-c66427543a00",
   "metadata": {},
   "source": [
    "### Loading Tokenizer and Encoding our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c006a-f70b-401b-9b82-1ee569cd106b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52f7c7-f5f8-4981-a8f6-1f2a9ac36aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93912f-25f5-4930-98be-40bb819827e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Tokenizer came from the Pre_trained BERT\n",
    "# 'bert-base-uncased' means that we are using all lower case data\n",
    "# `do_lower_case` Convert everything to lower-case.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a360c48-9431-4d40-8edc-a4b860a4eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding the Training data\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Encoding the Validation data\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Spliting the data for the BERT training\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368edea-7418-492b-845b-96ec6b883d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two different dataset\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a30e5e-1a69-4e5e-bbc7-1c03df21ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e5506-2a4a-49c2-9997-b70b1ddeddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafaf46b-e82d-4faa-b44d-7c9775e907ce",
   "metadata": {},
   "source": [
    "### Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1c66b-1c6e-44ba-8035-d228045aabe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f74547-d59a-4e80-a6c5-c4a778d4e467",
   "metadata": {},
   "source": [
    "### Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c598d8-556f-4eb6-a772-d03384419204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75b8ca-78d3-4f64-a63f-cdf5d3483849",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# We Need two different dataloder\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                              sampler=RandomSampler(dataset_val),\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10836b2e-449a-4052-b39d-06fb4f2d8b8c",
   "metadata": {},
   "source": [
    "### Setting Up Optimiser and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda60cd-3ba7-4d48-bd36-b33048c83729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf5e75-4a3e-44a5-976c-3e3c42994c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b61cd-44c8-401b-8e6f-9a01050b4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8a0d3-de8f-42e0-b848-85d768d7d28f",
   "metadata": {},
   "source": [
    "###  Defining our Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527b3e5-93e9-4dc6-a814-976be1fba89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1cddc-ae9e-4ecb-9432-6d76c3a42858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69e17c-e5bb-461f-972e-194e99aa0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "\n",
    "    # Setting up the preds to axis=1\n",
    "    # Flatting it to a single iterable list of array\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "\n",
    "    # Flattening the labels\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Returning the f1_score as define by sklearn\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2736b4d-0bdc-441b-845e-f2d70bc114fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Iterating over all the unique labels\n",
    "    # label_flat are the --> True labels\n",
    "    for label in np.unique(labels_flat):\n",
    "        # Taking out all the pred_flat where the True alable is the lable we care about.\n",
    "        # e.g. for the label Happy -- we Takes all Prediction for true happy flag\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f061e7-f70e-4f58-af1b-f43b1eb97b03",
   "metadata": {},
   "source": [
    "### Create a training loop to control PyTorch finetuning of BERT using CPU or GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b518b-0713-494c-84b8-36e52de5ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1236bed-de96-4f96-9aa7-f8e1c60a48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40bf52-6e13-4c1a-9b65-5583198bd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03c995-9fa4-422d-8b2e-45cacccd0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()          # Sending our model in Training mode\n",
    "    \n",
    "    loss_train_total = 0   # Setting the training loss to zero initially\n",
    "\n",
    "    # Setting up the Progress bar to Moniter the progress of training\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad() # As we not working with thew RNN's\n",
    "        \n",
    "        # As our dataloader has '3' iteams so batches will be the Tuple of '3'\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # INPUTS\n",
    "        # Pulling out the inputs in the form of dictionary\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        # OUTPUTS\n",
    "        outputs = model(**inputs) # '**' Unpacking the dictionary stright into the input\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()           # backpropagation\n",
    "\n",
    "        # Gradient Clipping -- Taking the Grad. & gives it a NORM value ~ 1 \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97b1d8-29af-4c0e-a28e-97fab15c3148",
   "metadata": {},
   "source": [
    "### Loading finetuned BERT model and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418c531-009e-453c-a2c6-02ccdb4bef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72906753-34d9-40d1-b042-bcaa32e03a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/finetuned_BERT_epoch_10.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03a892-2e23-4a35-b82a-e3c3809c75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3228fe-b4e5-4499-ba00-43e13d2e1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ee777-02cf-43b2-9f5c-5416c58f87ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
